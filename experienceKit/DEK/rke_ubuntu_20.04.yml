## This is the official DEK for RKE cluster with ESP profile.
##
## Preconditions:
## - Users need to setup the ESP network topologic (following the settings in config/extensions/esp_network.yml), and connect all the nodes to be installed to the ESP network.
## - Before running the "init" command, users need to:
##     - Input the MAC addresses and static IP addresses of the nodes in the "Parameters - nodes" config section.
##     - Input the default password of the nodes in the "Parameters - nodes" config section.
##     - Input the default SSH public key path. After ESP provisioning, allows you to connect to target nodes without password.
## - After OS provisioning finished with ESP, and before "cluster deploy", login to the nodes and make sure users have permission to run "sudo" command with no password.
##
## Features:
## - The ESP resource files will be installed from upstream with an external network connection.
## - The "cluster deploy" can be run in an offline mode (no external network connection needed).
## - The offline "service deploy" is not supported.

Use:
##import the configs set in common.yml
- experienceKit/DEK/common.yml

Parameters:
  customconfig:
    registry:
      ## set the password before running the command of "./conductor init -c *.yml"
      password:
  ## Input ssh public key path into the blank.
  ## Example: /home/path/.ssh/id_rsa.pub 
  ## default_ssh_key_path: /home/path/.ssh/id_rsa.pub
  default_ssh_key_path:
  ## Input http proxy and ESP will use this parameter to provision on target node
  ## Example: http_proxy: "http://www.example.com"
  ## After ESP provision, the http_proxy is already set on target node.
  ## So do https_proxy and no_proxy
  global_settings:
    http_proxy: ""
    https_proxy: ""
    no_proxy: ""
  ##nodes field contains a list of node objects.
  ## the below is a two-nodes cluster with one node supporting both control plane and etcd and
  ## one node supporting node.
  nodes:
  - mac:
    ip:
    role:
      - controlplane
      - etcd
    user: 
    ssh_passwd: 
  - mac:
    ip:
    role:
      - worker
    user:
    ssh_passwd: 
  ## to add a work (or controlplane) node by setting a group of attributor as below. 
  ## controlplane node is at least 1 and up to 3.
  ## ## Controlplane:
  ## - mac:
  ##  ip:
  ##  role:
  ##    - controlplane
  ##    - etcd
  ##  user: sys-admin
  ##  ssh_passwd:
  ##
  ## ## Worker:
  ## - mac:
  ##  ip:
  ##  role:
  ##    - worker
  ##  user: sys-admin
  ##  ssh_passwd:

  extensions:
  - esp_network
  - ingress
  - sriov
  - cpu-manager
  - service-tls

OS:
  manifests:
  - "config/manifests/os_provider_manifest.yml"
  provider: esp
  # Before running "init" with this EK config file, please update ESP config
  # with correct "git_username" and "git_token" to access the profile git repo.
  config: "config/os-provider/esp_config_profile-ubuntu-20.04.yml" 

Cluster:
  manifests:
  - "config/manifests/cluster_provider_manifest.yml"
  provider: rke
  config: "config/cluster-provider/rke_cluster.yml"
  # RKE config files will be exported under this folder.
  # Make sure the path is accessible.
  # Use absolute path or relative path.
  # DO NOT use "~" representing home directory.
  export_config_folder: {{ env "HOME" }}/.ec/rke/cluster

# To enable rook ceph, make sure there're at least 1 controller + 1 worker and
# a clean additional hard disk with >1GB storage must be available. 
Components:
  manifests:
  - "config/manifests/component_manifest.yml"
  selector:
  - name: nfd
  - name: intel-sriov-network
  - name: nginx-ingress
    override:
      url: https://github.com/kubernetes/ingress-nginx/releases/download/helm-chart-4.0.15/ingress-nginx-4.0.15.tgz
      type: helm
      chartoverride: file://{{ .Workspace }}/config/service-overrides/ingress/rke-nginx-ingress.yml
      supported-clusters:
      - rke
      images: []
  - name: rook-ceph
  - name: rook-ceph-cluster
  - name: portainer-ce

